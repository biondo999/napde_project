{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n!pip install pyDOE\n\nfrom pyDOE import lhs\n\nimport sys\nsys.path.insert(1, '/kaggle/input/quadrule')\n\nfrom GaussJacobiQuadRule_V3 import Jacobi, DJacobi, GaussLobattoJacobiWeights, GaussJacobiWeights\nimport time\nimport os\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VPINN(tf.keras.Model):\n\n    def __init__(self, pb, params,seed,NN):\n\n        super().__init__()\n\n        # accept parameters\n        self.pb = pb\n        self.params = params\n        self.SEED=seed\n\n        # generate all precomp stuff\n        self.generate_boundary_points()\n        self.generate_inner_points()\n        self.generate_quadrature_points()\n        self.construct_RHS()\n\n        # add the neural network to the class if given at initialisation\n        if NN:\n            self.set_NN(NN)\n        else: \n            self.NN=self.initialize_NN()\n            self.vars = self.NN.trainable_variables\n            self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.params['LR'])\n\n    def set_NN(self, NN, LR):\n        np.random.seed(self.SEED)\n        tf.random.set_seed(self.SEED)\n        # initialise the NN\n        self.NN = NN\n\n        # take trainable vars\n        self.vars = self.NN.trainable_variables\n\n        # set optimiser\n        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n\n    def initialize_NN(self):\n\n        NN = tf.keras.Sequential()\n        NN.add(tf.keras.layers.InputLayer(self.params['NN_struct'][0],dtype=tf.float64))\n        # NN.add(tf.keras.layers.Lambda(lambda x: 2. * (x + 1) / (2) - 1.))\n\n        for width in self.params['NN_struct'][1:-1]:\n            NN.add(tf.keras.layers.Dense(width,\n                                            activation=tf.math.sin,\n                                            use_bias=True,\n                                            kernel_initializer=tf.keras.initializers.GlorotUniform(self.SEED),\n                                            bias_initializer='zeros',dtype=tf.float64))\n        NN.add(tf.keras.layers.Dense(self.params['NN_struct'][-1], activation='linear',dtype=tf.float64,kernel_initializer=tf.keras.initializers.GlorotUniform(self.SEED)))\n\n        return NN\n\n\n    def eval_NN(self, x):\n        x = tf.convert_to_tensor(x, dtype=tf.float64)        \n\n        with tf.GradientTape() as second_order:\n            second_order.watch(x)\n            with tf.GradientTape() as first_order:\n                first_order.watch(x)\n                u = self.NN(x)\n            d1xu = first_order.gradient(u, x)\n        d2xu = second_order.gradient(d1xu, x)\n\n        del first_order\n        del second_order\n\n        return u, d1xu, d2xu\n\n    def boundary_loss(self):\n    ## NOTE:impose boundary or same structure for ICs\n        u_bound_NN = self.eval_NN(self.boundary_points)[0]\n        return tf.reduce_mean(tf.square(u_bound_NN - self.boundary_sol))\n\n    def variational_loss(self):\n        #TODO: general organisation\n        varloss_total = 0.0\n        N = self.params['n_elements']\n        for el in range(N):\n                \n            n_test = self.params['n_test'][el]\n\n            F_ext_element = self.F_ext_total[el]\n            jacobian = self.J[el]\n            x_quad_element = self.x_quad_total[el]\n            v_quad_element = self.v_quad_total[el]\n            dv_quad_element = self.dv_quad_total[el]\n            dv_boundary_element = self.dv_boundary_total[el]\n            d2v_quad_element = self.d2v_quad_total[el]\n\n\n            u_NN_quad_el, d1xu_NN_quad_el, d2xu_NN_quad_el = self.eval_NN(x_quad_element)\n            # dv_boundary_element, _ = self.pb.dtest_func(n_test, np.array([-1, 1]))\n\n            if self.params['var_form'] == 0:\n                u_NN_el = tf.stack([-jacobian*tf.reduce_sum(self.w_quad*d2xu_NN_quad_el*v_quad_element[i]) \\\n                                    for i in range(n_test)])\n\n            elif self.params['var_form'] == 1:\n                u_NN_el = tf.stack([tf.reduce_sum(self.w_quad*d1xu_NN_quad_el*dv_quad_element[i]) \\\n                                    for i in range(n_test)])\n                \n\n            elif self.params['var_form'] == 2:\n                u_NN_bound_el, _, _  = self.eval_NN(np.array([x_quad_element[0], x_quad_element[-1]]))\n                u_NN_el = tf.stack([-1/jacobian*tf.reduce_sum(self.w_quad*u_NN_quad_el*d2v_quad_element[i]) \\\n                                    +1/jacobian*tf.reduce_sum(u_NN_bound_el*np.array([-dv_boundary_element[i][0], dv_boundary_element[i][-1]]))  \\\n                                    for i in range(n_test)])\n            \n\n            res_NN_element = u_NN_el - F_ext_element\n            loss_element = tf.reduce_mean(tf.square(res_NN_element))\n            varloss_total = varloss_total + loss_element\n\n\n        return varloss_total\n\n    @tf.function\n    def loss_total(self):\n        loss_0 = 0.0\n        loss_b = self.boundary_loss()\n        loss_v = self.variational_loss()\n        loss_tot = loss_0 + loss_b + loss_v\n        return loss_tot, loss_b, loss_v\n\n    def loss_gradient(self):\n        with tf.GradientTape(persistent=True) as loss_grad:\n            #TODO: why is this commented?\n            # loss_grad.watch(self.vars)\n            loss_tot, loss_b, loss_v = self.loss_total()\n        gradient = loss_grad.gradient(loss_tot, self.vars)\n        return loss_tot, loss_b, loss_v, gradient\n\n    @tf.function\n    def gradient_descent(self):\n        loss_tot, loss_b, loss_v, gradient = self.loss_gradient()\n        self.optimizer.apply_gradients(zip(gradient, self.vars))\n        return loss_tot, loss_b, loss_v\n\n    def train(self, opt_iter):\n\n        history = []\n\n        start_time = time.time()\n        for i in range(opt_iter):\n\n            loss_tot, loss_b, loss_v = self.gradient_descent()\n\n            if i % 10 == 0:\n                elapsed = time.time() - start_time\n                print(f'Iteration: {i}', f'loss: {loss_tot.numpy():0.6f}', f'time: {elapsed}')\n                history.append([i, loss_tot, loss_b, loss_v])\n                start_time = time.time()\n\n        history = np.array(history)\n        self.iterations = history[:,0]\n        self.loss_tot = history[:,1]\n        self.loss_b = history[:,2]\n        self.loss_v = history[:,3]\n\n        return self.loss_tot\n\n    def get_domain_info(self):\n\n        a = self.params['domain'][0]\n        b = self.params['domain'][1]\n\n        scale = b - a\n        mid = (a + b)*0.5\n\n        return a, b, scale, mid\n\n    def generate_boundary_points(self):\n        # Boundary points\n        a, b, scale, mid = self.get_domain_info()\n\n        self.boundary_points = np.asarray([a,b], dtype=np.float64)[:,None]\n        self.boundary_sol = self.pb.u_exact(self.boundary_points)\n\n\n    def generate_inner_points(self):\n        _, _, scale, mid = self.get_domain_info()\n\n        self.X_f_train = np.array(scale*(lhs(1, self.params['n_bound']) - 0.5) + mid).flatten()\n        self.f_train = np.asarray(self.pb.f_exact(self.X_f_train))\n        # self.f_train = ff[:, None]\n\n    def generate_quadrature_points(self):\n        [self.x_quad, self.w_quad] = GaussLobattoJacobiWeights(self.params['n_quad'], 0, 0)\n\n\n\n    def construct_RHS(self):\n        #TODO: check are the convert to tensors are needed\n        N = self.params['n_elements']\n        lower_bound, upper_bound, _, _ = self.get_domain_info()\n        dx = (upper_bound - lower_bound)/N\n        self.grid_x = np.asarray([lower_bound + i*dx for i in range(N+1)])\n        n_test_funcs = self.params['n_test']\n\n        self.U_ext_total = []\n        self.F_ext_total = []\n        self.J = []\n        self.x_quad_total = []\n        self.v_quad_total = []\n        self.dv_quad_total = []\n        self.dv_boundary_total = []\n        self.d2v_quad_total = []\n\n        for el_x in range(N):\n            Ntest_element = n_test_funcs[el_x]\n\n            a = self.grid_x[el_x]\n            b = self.grid_x[el_x+1]\n\n            jacobian = (b - a)/2\n            x_quad_element = a + jacobian*(self.x_quad+1)\n\n            v_quad_element = self.pb.test_func(Ntest_element, self.x_quad)        \n            dv_quad_element, d2v_quad_element = self.pb.dtest_func(Ntest_element, self.x_quad)\n            dv_boundary_element, _ = self.pb.dtest_func(Ntest_element, np.array([a, b]))\n\n            u_quad_element = self.pb.u_exact(x_quad_element)\n            f_quad_element = self.pb.f_exact(x_quad_element)\n\n            U_ext_element = np.asarray([jacobian*np.sum(self.w_quad*v_quad_element[r]*u_quad_element)\n                for r in range(Ntest_element)])\n\n            F_ext_element = np.asarray([jacobian*np.sum(self.w_quad*v_quad_element[r]*f_quad_element)\n                for r in range(Ntest_element)])\n\n\n            self.U_ext_total.append(U_ext_element)\n            self.F_ext_total.append(F_ext_element)\n            self.J.append(jacobian)\n            self.x_quad_total.append(x_quad_element)\n            self.v_quad_total.append(v_quad_element)\n            self.dv_quad_total.append(dv_quad_element)\n            self.dv_boundary_total.append(dv_boundary_element)\n            self.d2v_quad_total.append(d2v_quad_element)\n\n            \n        self.F_ext_total = tf.convert_to_tensor(self.F_ext_total, tf.float64)\n\n    def generate_test_points(self):\n        lower_bound, upper_bound, _, _ = self.get_domain_info()\n\n        delta_test = self.params['delta_test']\n        x_test = np.arange(lower_bound, upper_bound + delta_test, delta_test)\n        data_temp = np.asarray([[x_test[i], self.pb.u_exact(x_test[i])]\n                                 for i in range(len(x_test))])\n    \n        x_test = data_temp.flatten()[0::2]\n        exact = data_temp.flatten()[1::2]\n        return x_test[:, None], exact[:, None], len(x_test)\n    \n\n    def plot_loss_history(self, PLOT='SHOW'):\n\n        font = 24\n        fig, ax = plt.subplots()\n        plt.tick_params(axis='y', which='both', labelleft='on', labelright='off') \n        plt.xlabel('$iteration$', fontsize = font)\n        plt.ylabel('$loss \\,\\, values$', fontsize = font)\n        plt.yscale('log')\n        plt.grid(True)\n        plt.plot(self.iterations, self.loss_tot,'green',label=\"loss\")\n        plt.legend(loc=\"upper right\")\n        plt.tick_params( labelsize = 20)\n        fig.set_size_inches(w=11,h=5.5)\n        plt.show()\n        \n        fig, ax = plt.subplots()\n        plt.tick_params(axis='y', which='both', labelleft='on', labelright='off')\n        plt.xlabel('$iteration$', fontsize = font)\n        plt.ylabel('$loss \\,\\, values$', fontsize = font)\n        plt.yscale('log')\n        plt.grid(True)\n        plt.plot(self.iterations, self.loss_tot,'green',label=\"loss\")\n        plt.plot(self.iterations, self.loss_b,'blue',label=\"boundary_loss\")\n        plt.plot(self.iterations, self.loss_v, 'violet',label=\"variational_loss\")\n        plt.legend(loc=\"upper right\")\n        plt.tick_params( labelsize = 20)\n        fig.set_size_inches(w=11,h=5.5)\n        plt.show()\n\n        if PLOT == 'save':\n            plt.savefig('VPINN_loss_history.pdf')\n        else:\n            plt.show()\n        \n\n    def plot_predict(self, PLOT='SHOW'):\n\n        x, sol, n_points = self.generate_test_points()\n        prediction = self.eval_NN(x)[0]\n        \n        pnt_skip = 25\n        fig, ax = plt.subplots()\n        plt.locator_params(axis='x', nbins=6)\n        plt.locator_params(axis='y', nbins=8)\n\n        plt.xlabel('$x$', fontsize = 24)\n        plt.ylabel('$u$', fontsize = 24)\n        plt.axhline(0, linewidth=0.8, linestyle='-', color='gray')\n        for xc in self.grid_x:\n            plt.axvline(x=xc, linewidth=2, ls = '--')\n        plt.plot(x, sol, linewidth=1, color='r', label=''.join(['$exact$']))\n        plt.plot(x, prediction, 'k*', label='$VPINN$')\n        plt.tick_params( labelsize = 20)\n        legend = plt.legend(shadow=True, loc='upper left', fontsize=18, ncol = 1)\n        fig.set_size_inches(w=11,h=5.5)\n        plt.show()\n\n        if PLOT == 'save':\n            plt.savefig('Exact.png')\n        else:\n            plt.show()\n        \n        \n    def plot_pointwise_error(self, PLOT='SHOW'):\n\n        x, sol, n_points = self.generate_test_points()\n\n        prediction = self.eval_NN(x)[0]\n\n        fig, ax = plt.subplots()\n        plt.locator_params(axis='x', nbins=6)\n        plt.locator_params(axis='y', nbins=8)\n        plt.xlabel('$x$', fontsize = 24)\n        plt.ylabel('point-wise error', fontsize = 24)\n        plt.yscale('log')\n        plt.axhline(0, linewidth=0.8, linestyle='-', color='gray')\n        for xc in self.grid_x:\n            plt.axvline(x=xc, linewidth=2, ls = '--')\n        plt.plot(x, abs(sol - prediction), 'k')\n        plt.tick_params( labelsize = 20)\n        fig.set_size_inches(w=11,h=5.5)\n        plt.show()\n\n        if PLOT == 'save':\n            plt.savefig('Pointwise_Error.png')\n        else:\n            plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PROBDEF:\n\n    def __init__(self, omega: tuple, r: int):\n        self.omega = omega\n        self.r = r\n\n    def u_exact(self, x):\n        utemp = 0.1*np.sin(self.omega*x) + np.tanh(self.r*x)\n        return utemp\n\n    def f_exact(self, x):\n        A =  0.1*(self.omega**2)*np.sin(self.omega*x)\n        B = (2*self.r**2)*(np.tanh(self.r*x))\n        B /= (np.cosh(self.r*x)); B /= (np.cosh(self.r*x))\n        return A + B\n\n    @staticmethod\n    def test_func_core(n, x):\n        test = Jacobi(n+1, 0, 0, x) - Jacobi(n-1, 0, 0, x)\n        return test\n\n    def test_func(self, n_test, x):\n        test_total = [self.test_func_core(n, x) for n in range(1, n_test+1)]\n        return np.asarray(test_total)\n\n    def dtest_func(self, n_test, x):\n        n = 1\n        d1test_total = [((n+2)/2)*Jacobi(n, 1, 1, x)]\n        d2test_total = [((n+2)*(n+3)/(2*2))*Jacobi(n-1, 2, 2, x)]\n        for n in range(2, n_test+1):\n            if n == 2:\n                d1test = ((n+2)/2)*Jacobi(n, 1, 1, x) - ((n)/2)*Jacobi(n-2, 1, 1, x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1, 2, 2, x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)\n            elif n > 2:\n                d1test = ((n+2)/2)*Jacobi(n, 1, 1, x) - ((n)/2)*Jacobi(n-2, 1, 1, x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1, 2, 2, x) - ((n)*(n+1)/(2*2))*Jacobi(n-3, 2, 2, x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)\n            else:\n                raise ValueError(\"Please check the value for 'n_test'\")\n        return np.asarray(d1test_total), np.asarray(d2test_total)\n    \n    \n'''\nHyper-parameters: \n    scheme     = is either 'PINNs' or 'VPINNs'\n    Net_layer  = the structure of fully connected network\n    var_form   = the form of the variational formulation used in VPINNs\n                    0, 1, 2: no, once, twice integration-by-parts\n    N_el_x, N_el_y     = number of elements in x and y direction\n    N_test_x, N_test_y = number of test functions in x and y direction\n    N_quad     = number of quadrature points in each direction in each element\n    N_bound    = number of boundary points in the boundary loss\n    N_residual = number of residual points in PINNs\n'''","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pb = PROBDEF((8*np.pi), 80)\nN_tests = 20\nN_elements = 5\nparams = {'scheme': 'VPINNs',\n            'NN_struct': [1] + [20] * 4 + [1],\n            'var_form': 1,\n            'n_elements': N_elements,\n            'n_test': N_elements*[N_tests],\n            'n_quad': 100,\n            'n_bound': 50,\n            'n_residual': 100,\n            'domain': (-1, 1),\n            'Opt_Niter': 2500 + 1,\n            'delta_test': 0.01,\n            'LR': 0.001}\nprint(params)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PLOT = 'show'\n\n#changing seed\nfrom random import *\n\nseeds = [randint(1, 100) for iter in range(5)] \n\n\nfor seed in seeds:\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\n    model = VPINN(pb,params,seed,None)\n    print(\"Seed : \",seed)\n    model.plot_predict(PLOT)\n\n    loss_his = model.train(params['Opt_Niter'])\n\n    model.plot_predict(PLOT)\n    model.plot_loss_history(PLOT)    \n    model.plot_pointwise_error(PLOT)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model.NN.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(params['NN_struct'][0])\ninput=tf.constant([[1.0],[2.0]],dtype=tf.float64)\nprint(input)\n\n#the input must be a tensor of two dims(col)\n#if you inherit from tf.keras model and you init like you did you can view the structure of the the net if you type:\nprint(model.NN(input))\n","metadata":{},"execution_count":null,"outputs":[]}]}