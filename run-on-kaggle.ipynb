{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyDOE\n#  !pip install all other things if it gives you errors","metadata":{"execution":{"iopub.execute_input":"2023-04-09T19:31:01.182223Z","iopub.status.busy":"2023-04-09T19:31:01.181802Z","iopub.status.idle":"2023-04-09T19:31:15.281769Z","shell.execute_reply":"2023-04-09T19:31:15.280498Z"},"papermill":{"duration":14.108476,"end_time":"2023-04-09T19:31:15.284648","exception":false,"start_time":"2023-04-09T19:31:01.176172","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#you need to add a dataset with GaussJacobiQuadRule_V3 on the right \nimport sys\nsys.path.insert(1, '/kaggle/input/quadrule')\nfrom GaussJacobiQuadRule_V3 import Jacobi, DJacobi, GaussLobattoJacobiWeights, GaussJacobiWeights\n\n#import GaussJacobiQuadRule_V3","metadata":{"execution":{"iopub.execute_input":"2023-04-09T19:31:15.293944Z","iopub.status.busy":"2023-04-09T19:31:15.293184Z","iopub.status.idle":"2023-04-09T19:31:15.515301Z","shell.execute_reply":"2023-04-09T19:31:15.513886Z"},"papermill":{"duration":0.229814,"end_time":"2023-04-09T19:31:15.518030","exception":false,"start_time":"2023-04-09T19:31:15.288216","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %load building_VPINN_network.py\n\n\n###############################################################################\n\n# import tensorflow as tf\n\nimport tensorflow \ntf=tensorflow.compat.v1\n#tf.disable_v2_behavior()\nimport pyDOE\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom pyDOE import lhs\nimport time\n\n#from tensorflow import placeholder\n\n\nnp.random.seed(1234)\ntf.compat.v1.set_random_seed(1234)\n\n###############################################################################\n\n                  #X_u_train,u_train,               points for training at the boundary \n                  #X_quad_train, W_quad_train,      quadrature weights and points in [-1,1](N_quad is their number) arrays\n                  #F_ext_total,                     fh calculated on each elem for each test function on the real nodes array \n                  #grid,                            lispace grid between -1 and 1 with N_quad number of points points \n                  #X_test,u_test     \t\t    test points between -1 and 1 depends on delta_test and uex evalueted on these\n                  #Net_layer         \t\t    array of integer with number of neurons for each layer\n                  #X_f_train,                       Nf points for trianing,then evalueted on f\n                  #f_train,                         \n                  #params=params                    hyperparameters of the network/loss options \n\nclass VPINN:\n    def __init__(self, X_u_train, u_train, X_quad, W_quad, F_exact_total,\\\n                 grid, X_test, u_test, layers, X_f_train, f_train, params):\n\n        self.x       = X_u_train\n        self.u       = u_train\n        \n        self.xf      = X_f_train\n        self.f      = f_train\n        \n        self.xquad   = X_quad\n        self.wquad   = W_quad  #weights for quadrature to calculate the loss\n        \n        self.xtest   = X_test\n        self.utest   = u_test\n        \n        self.F_ext_total = F_exact_total\n        self.Nelement = np.shape(self.F_ext_total)[0]  #number of elements  \n        self.N_test   = np.shape(self.F_ext_total[0])[0] #number of test function\n        \n\n\n\n        self.x_tf   = tf.placeholder(tf.float64, shape=[None, self.x.shape[1]]) #????placeholder is a variable that we will assign later on\n        self.u_tf   = tf.placeholder(tf.float64, shape=[None, self.u.shape[1]])\n        self.xf_tf   = tf.placeholder(tf.float64, shape=[None, self.xf.shape[1]])\n        self.f_tf   = tf.placeholder(tf.float64, shape=[None, self.f.shape[1]])\n        self.x_test = tf.placeholder(tf.float64, shape=[None, self.xtest.shape[1]])\n        self.x_quad = tf.placeholder(tf.float64, shape=[None, self.xquad.shape[1]])\n      \n        self.weights, self.biases, self.a = self.initialize_NN(layers)\n        \n        #ingredients for discrete integrals,remark the newtork is not just one you need to calculate in poisson case u(x,y) ,d/dx u(x,y) ,d/dy u(x,y)\n        \n        self.u_NN_quad  = self.net_u(self.x_quad) #x_quad is a row tensor \n        \n        self.d1u_NN_quad, self.d2u_NN_quad = self.net_du(self.x_quad)\n         \n       \tself.test_quad=self.Test_fcn(self.N_test, self.xquad)\n       \t\n        self.d1test_quad, self.d2test_quad = self.dTest_fcn(self.N_test, self.xquad)\n        \n \n        \n        self.u_NN_pred   = self.net_u(self.x_tf)\n        self.u_NN_test   = self.net_u(self.x_test)\n        self.f_pred = self.net_f(self.x_test) #evaluete f through the network ??\n        \n        \n        #start from there -> loss calculation\n        self.varloss_total = 0\n        for e in range(self.Nelement):\n            F_ext_element  = self.F_ext_total[e]\n            Ntest_element  = np.shape(F_ext_element)[0] #for each element of the grid you have a vector of the focing term (suppose its n-loc),so the you can have at most quad formula n_loc\n            \n            x_quad_element = tf.constant(grid[e] + (grid[e+1]-grid[e])/2*(self.xquad+1))\n            x_b_element    = tf.constant(np.array([[grid[e]], [grid[e+1]]]))\n            #to change change integral to the ref segment in (-1,1)\n            jacobian       = (grid[e+1]-grid[e])/2\n\n            test_quad_element = self.Test_fcn(Ntest_element, self.xquad)\n            d1test_quad_element, d2test_quad_element = self.dTest_fcn(Ntest_element, self.xquad)\n            u_NN_quad_element = self.net_u(x_quad_element)\n            d1u_NN_quad_element, d2u_NN_quad_element = self.net_du(x_quad_element)\n\n            u_NN_bound_element = self.net_u(x_b_element)\n            d1test_bound_element, d2test_bounda_element = self.dTest_fcn(Ntest_element, np.array([[-1],[1]]))\n\n            var_form = params['var_form']\n\n            if var_form == 1:\n                U_NN_element = tf.reshape(tf.stack([-jacobian*tf.reduce_sum(self.wquad*d2u_NN_quad_element*test_quad_element[i]) \\\n                                                   for i in range(Ntest_element)]),(-1,1))\n            if var_form == 2:\n                U_NN_element = tf.reshape(tf.stack([ tf.reduce_sum(self.wquad*d1u_NN_quad_element*d1test_quad_element[i]) \\\n                                                    for i in range(Ntest_element)]),(-1,1))                                 #i think we are going to use this most of the times \n            if var_form == 3:\n                U_NN_element = tf.reshape(tf.stack([-1/jacobian*tf.reduce_sum(self.wquad*u_NN_quad_element*d2test_quad_element[i]) \\\n                                                   +1/jacobian*tf.reduce_sum(u_NN_bound_element*np.array([-d1test_bound_element[i][0], d1test_bound_element[i][-1]]))  \\\n                                                   for i in range(Ntest_element)]),(-1,1))\n                \n\n            Res_NN_element = U_NN_element - F_ext_element\n            loss_element = tf.reduce_mean(tf.square(Res_NN_element))\n            self.varloss_total = self.varloss_total + loss_element\n        \n        self.lossb = tf.reduce_mean(tf.square(self.u_tf - self.u_NN_pred))  #u_NN_pred is what your network has calc,while u_tf is the real value\n        self.lossv = self.varloss_total\n        #two losses \n        self.loss  = params['lossb_weight']*self.lossb + self.lossv\n        \n        self.LR = params['LR']\n        self.optimizer_Adam = tf.train.AdamOptimizer(self.LR)\n        self.train_op_Adam = self.optimizer_Adam.minimize(self.loss)\n        self.sess = tf.Session(config=tf.ConfigProto(device_count={'GPU': 0}))\n        self.init = tf.global_variables_initializer()\n        self.sess.run(self.init)\n\n###############################################################################\n    def initialize_NN(self, layers):        \n        weights = []\n        biases = []\n        num_layers = len(layers) \n        #transpose everything to have the classic form y=W*x+b \n        for l in range(0,num_layers-1):\n            W = self.xavier_init(size=[layers[l], layers[l+1]])\n            b = tf.Variable(tf.zeros([1,layers[l+1]], dtype=tf.float64), dtype=tf.float64)\n            a = tf.Variable(0.01, dtype=tf.float64)\n            weights.append(W)\n            biases.append(b)        \n        return weights, biases, a\n        \n        \n    def xavier_init(self, size): #marked for future develop*********************\n        in_dim = size[0]\n        out_dim = size[1]        \n        xavier_stddev = np.sqrt(2/(in_dim + out_dim), dtype=np.float64)\n        return tf.Variable(tf.random.truncated_normal([in_dim, out_dim], stddev=xavier_stddev,dtype=tf.float64), dtype=tf.float64) #maybe should the init be changed\n \n    def neural_net(self, X, weights, biases, a): #marked for future develop \n        num_layers = len(weights) + 1\n        H = X \n        for l in range(0,num_layers-2):\n            W = weights[l]\n            b = biases[l]\n            H = tf.sin(tf.add(tf.matmul(H, W), b)) #change here for having different activation function\n        W = weights[-1]\n        b = biases[-1]\n        Y = tf.add(tf.matmul(H, W), b)\n        return Y\n\n    def net_u(self, x):  #marked for future develop\n        u = self.neural_net(tf.concat([x],1), self.weights, self.biases, self.a)\n        return u\n\n    def net_du(self, x): #calculates first and second derivatives of the input unn d/dx unn d^2/dx^2 unn so it can compute loss\n        u   = self.net_u(x)\n        d1u = tf.gradients(u, x)[0]\n        d2u = tf.gradients(d1u, x)[0]\n        return d1u, d2u\n\n    def net_f(self, x): #marked for future develop*********************\n        u = self.net_u(x)\n        u_x = tf.gradients(u, x)[0]\n        u_xx = tf.gradients(u_x, x)[0]\n        f = - u_xx\n        return f\n    \n    #vtest functions == jacobi polynomils valueted on a point x\n    def Test_fcn(self, N_test,x):\n        test_total = []\n        for n in range(1,N_test+1):  \n            test  = Jacobi(n+1,0,0,x) - Jacobi(n-1,0,0,x)\n            test_total.append(test)\n        return np.asarray(test_total)\n\n    def dTest_fcn(self, N_test,x):  #valuete the first and second derivatives of test functions on a point x \n        d1test_total = []\n        d2test_total = []\n        for n in range(1,N_test+1):  \n            if n==1:\n                d1test = ((n+2)/2)*Jacobi(n,1,1,x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1,2,2,x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)\n            elif n==2:\n                d1test = ((n+2)/2)*Jacobi(n,1,1,x) - ((n)/2)*Jacobi(n-2,1,1,x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1,2,2,x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)    \n            else:\n                d1test = ((n+2)/2)*Jacobi(n,1,1,x) - ((n)/2)*Jacobi(n-2,1,1,x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1,2,2,x) - ((n)*(n+1)/(2*2))*Jacobi(n-3,2,2,x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)    \n        return np.asarray(d1test_total), np.asarray(d2test_total)\n\n    def predict_subdomain(self, grid):\n        error_u_total = []\n        u_pred_total = []\n        for e in range(self.Nelement):\n            utest_element = self.utest_total[e]\n            x_test_element = grid[e] + (grid[e+1]-grid[e])/2*(self.xtest+1)\n            u_pred_element = self.sess.run(self.u_NN_test, {self.x_test: x_test_element})\n            error_u_element = np.linalg.norm(utest_element - u_pred_element,2)/np.linalg.norm(utest_element,2)\n            error_u_total.append(error_u_element)\n            u_pred_total.append(u_pred_element)\n        return u_pred_total, error_u_total\n\n    def predict(self, x):\n        u_pred  = self.sess.run(self.u_NN_test, {self.x_test: x})\n        return u_pred        \n\n    def train(self, nIter, tresh, total_record):\n        \n        tf_dict = {self.x_tf: self.x, self.u_tf: self.u,\\\n                   self.x_quad: self.xquad, self.x_test: self.xtest,\\\n                   self.xf_tf: self.xf, self.f_tf: self.f}    #maybe like variables sess.run will do the rest, u(boundary cond) is assigned to u_tf ,x(boundary) is assigned to x_tf and so on\n        start_time       = time.time()\n        for it in range(nIter):\n            self.sess.run(self.train_op_Adam, tf_dict)\n            #each 10 iter saves loss values\n            if it % 10 == 0:\n                loss_value = self.sess.run(self.loss, tf_dict)\n                loss_valueb= self.sess.run(self.lossb, tf_dict)\n                loss_valuev= self.sess.run(self.lossv, tf_dict)\n                total_record.append(np.array([it, loss_value,loss_valueb,loss_valuev]))\n                \n                if loss_value < tresh:\n                    print('It: %d, Loss: %.3e' % (it, loss_value))\n                    break\n                \n            if it % 100 == 0:\n                elapsed = time.time() - start_time\n                str_print = 'It: %d, Lossb: %.3e, Lossv: %.3e, Time: %.2f'\n                print(str_print % (it, loss_valueb, loss_valuev, elapsed))\n                start_time = time.time()\n\n        return total_record\n","metadata":{"execution":{"iopub.execute_input":"2023-04-09T19:31:15.527204Z","iopub.status.busy":"2023-04-09T19:31:15.526736Z","iopub.status.idle":"2023-04-09T19:31:24.566579Z","shell.execute_reply":"2023-04-09T19:31:24.565349Z"},"papermill":{"duration":9.048032,"end_time":"2023-04-09T19:31:24.569469","exception":false,"start_time":"2023-04-09T19:31:15.521437","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntf.compat.v1.disable_v2_behavior()\n# from tensorflow import placeholder\n\n                    # a(u,v)=f(v) u in H1 such that is 0 on diriclet boundary for every v in V -->ah(u,vh)=fh(vh) for every vh in Vh\n\n    \n    #++++++++++++++++++++++++++++\nLR = 0.001\nOpt_Niter = 5000 + 1\nOpt_tresh = 2e-32\nvar_form  = 2  #i think that makes the most sense leaving it to 2(variational form)\nN_Element = 5\nNet_layer = [1] + [20] * 4 + [1] # [1 20 20 20 20 1] number of neurons in each later numer of param == (input_layer+1)*output_layer in this case 2*20+3*(21*20)+21*1=1321 total param\nN_testfcn = 20 #number of test function\nN_Quad = 100    #quadrature points \nN_F = 500      #number of points used for trianing \nlossb_weight = 1 #hyprparameter we may turn it up\n\nparams = {'var_form': var_form, 'lossb_weight': lossb_weight, 'LR': LR}\n    \n#++++++++++++++++++++++++++++    valuete Test function of order n in a point x(n>=1)\ndef Test_fcn(n,x):\n    test  = Jacobi(n+1,0,0,x) - Jacobi(n-1,0,0,x)\n    return test\n\n#++++++++++++++++++++++++++++    \n#exact sol and forcing term\nomega = 8*np.pi\namp = 1\nr1 = 80\ndef u_ext(x):\n    utemp = 0.1*np.sin(omega*x) + np.tanh(r1*x)\n    return amp*utemp\n\ndef f_ext(x):\n    gtemp =  -0.1*(omega**2)*np.sin(omega*x) - (2*r1**2)*(np.tanh(r1*x))/((np.cosh(r1*x))**2)\n    return -amp*gtemp\n\n#++++++++++++++++++++++++++++ generate once for all quad nodes and weights in the interval [-1,1]\nNQ_u = N_Quad\n[x_quad, w_quad] = GaussLobattoJacobiWeights(NQ_u, 0, 0)\ntestfcn = np.asarray([ Test_fcn(n,x_quad)  for n in range(1, N_testfcn+1)])\n\n#generate grid elements,for each element the number of test fuction is the same  \nNE = N_Element\n[x_l, x_r] = [-1, 1]\ndelta_x = (x_r - x_l)/NE\ngrid = np.asarray([ x_l + i*delta_x for i in range(NE+1)])\nN_testfcn_total = np.array((len(grid)-1)*[N_testfcn])\n\nif N_Element == 3:\n    grid = np.array([-1, -0.1, 0.1, 1])\n    NE = len(grid)-1\n    N_testfcn_total = np.array([N_testfcn,N_testfcn,N_testfcn])\n\nU_ext_total = []\nF_ext_total = []\nfor e in range(NE):\n    x_quad_element = grid[e] + (grid[e+1]-grid[e])/2*(x_quad+1)\n    jacobian = (grid[e+1]-grid[e])/2\n    N_testfcn_temp = N_testfcn_total[e]\n    testfcn_element = np.asarray([ Test_fcn(n,x_quad)  for n in range(1, N_testfcn_temp+1)])\n    \n    #is this my left side ot the variational form ah calculated in the real nodal values ?  \n    u_quad_element = u_ext(x_quad_element)\n    U_ext_element  = jacobian*np.asarray([sum(w_quad*u_quad_element*testfcn_element[i]) for i in range(N_testfcn_temp)])\n    U_ext_element = U_ext_element[:,None]\n    U_ext_total.append(U_ext_element)\n    #this is fh calculated on the real nodal values summed over each real nodal values(which comes form CGL nodes )\n    f_quad_element = f_ext(x_quad_element)\n    F_ext_element  = jacobian*np.asarray([sum(w_quad*f_quad_element*testfcn_element[i]) for i in range(N_testfcn_temp)])\n    F_ext_element = F_ext_element[:,None]\n    F_ext_total.append(F_ext_element)\n\n# at the end i have an array with my all my \"real\" residues\nU_ext_total = np.asarray(U_ext_total)\nF_ext_total = np.asarray(F_ext_total)\n\n#++++++++++++++++++++++++++++\n# Training points\nX_u_train = np.asarray([-1.0,1.0])[:,None]\nu_train   = u_ext(X_u_train)\nX_bound = np.asarray([-1.0,1.0])[:,None]\n\nNf = N_F\nX_f_train = (2*lhs(1,Nf)-1) #generate random number of training points \nf_train   = f_ext(X_f_train) #evaluete these points on f(forcing therm)\n\n#++++++++++++++++++++++++++++\n# Quadrature points\n[x_quad, w_quad] = GaussLobattoJacobiWeights(N_Quad, 0, 0)\n\nX_quad_train = x_quad[:,None] #[[w1],[w2]] none stands for new axis\nW_quad_train = w_quad[:,None]\n\n#++++++++++++++++++++++++++++\n# Test point\ndelta_test = 0.001\nxtest      = np.arange(-1 , 1 + delta_test , delta_test)  #linspace\ndata_temp  = np.asarray([ [xtest[i],u_ext(xtest[i])] for i in range(len(xtest))]) # pair input real ouput \nX_test = data_temp.flatten()[0::2]\nu_test = data_temp.flatten()[1::2]\n#test values \nX_test = X_test[:,None] \nu_test = u_test[:,None] \nf_test = f_ext(X_test)\n\nu_test_total = []\nfor e in range(NE):\n    x_test_element = grid[e] + (grid[e+1]-grid[e])/2*(xtest+1)\n    u_test_element = u_ext(x_test_element)\n    u_test_element = u_test_element[:,None]\n    u_test_total.append(u_test_element)\n\n#++++++++++++++++++++++++++++\n# Model and Training\nmodel = VPINN(X_u_train, u_train, X_quad_train, W_quad_train, F_ext_total,\\\n                grid, X_test, u_test, Net_layer, X_f_train, f_train, params=params)\n                #X_u_train,u_train,               points for training at the boundary \n                #X_quad_train, W_quad_train,      quadrature weights and points in [-1,1](N_quad is their number) arrays\n                #F_ext_total,                     fh calculated on each elem for each test function on the real nodes array\n                #grid,                            lispace grid between -1 and 1 with N_quad number of points points \n                #X_test,u_test     \t\t    test points between -1 and 1 depends on delta_test and uex evalueted on these\n                #Net_layer         \t\t    array of integer with number of neurons for each layer\n                #X_f_train,                       Nf points for trianing,then evalueted on f\n                #f_train,                         \n                #params=params                    hyperparameters of the network/loss options \n\ntotal_record = model.train(Opt_Niter, Opt_tresh, [])\nu_pred = model.predict(X_test)\n    \n    \n    \n    \n    \n    #dont care for the moment\n    # =========================================================================\n    #     Plotting\n    # =========================================================================    \nx_quad_plot = X_quad_train\ny_quad_plot = np.empty(len(x_quad_plot))\ny_quad_plot.fill(1)\n\nx_train_plot = X_u_train\ny_train_plot = np.empty(len(x_train_plot))\ny_train_plot.fill(1) \n\nx_f_plot = X_f_train\ny_f_plot = np.empty(len(x_f_plot))\ny_f_plot.fill(1)\n\n\n\n\n\n\n\nfig = plt.figure(0)\ngridspec.GridSpec(3,1)\n\nplt.subplot2grid((3,1), (0,0))\nplt.tight_layout()\nplt.locator_params(axis='x', nbins=6)\nplt.yticks([])\nplt.title('$Quadrature \\,\\, Points$')\nplt.xlabel('$x$')\nplt.axhline(1, linewidth=1, linestyle='-', color='red')\nplt.axvline(-1, linewidth=1, linestyle='--', color='red')\nplt.axvline(1, linewidth=1, linestyle='--', color='red')\nplt.scatter(x_quad_plot,y_quad_plot, color='green')\n\nplt.subplot2grid((3,1), (1,0))\nplt.tight_layout()\nplt.locator_params(axis='x', nbins=6)\nplt.yticks([])\nplt.title('$Training \\,\\, Points$')\nplt.xlabel('$x$')\nplt.axhline(1, linewidth=1, linestyle='-', color='red')\nplt.axvline(-1, linewidth=1, linestyle='--', color='red')\nplt.axvline(1, linewidth=1, linestyle='--', color='red')\n#the data points are only the boundary points!!!!\nplt.scatter(x_train_plot,y_train_plot, color='blue')\n\nfig.tight_layout()\nfig.set_size_inches(w=10,h=7)\nplt.show()\n# plt.savefig('Train-Quad-pnts.pdf')    \n#++++++++++++++++++++++++++++\n\nfont = 24\n#all loss\nfig, ax = plt.subplots()\nplt.tick_params(axis='y', which='both', labelleft='on', labelright='off') \nplt.xlabel('$iteration$', fontsize = font)\nplt.ylabel('$loss \\,\\, values$', fontsize = font)\nplt.yscale('log')\nplt.grid(True)\niteration = [total_record[i][0] for i in range(len(total_record))]\nloss_his  = [total_record[i][1] for i in range(len(total_record))]\nplt.plot(iteration, loss_his,'green',label=\"loss\")\nplt.legend(loc=\"upper right\")\nplt.tick_params( labelsize = 20)\nfig.set_size_inches(w=11,h=5.5)\nplt.show()\n# plt.savefig('loss.pdf')\n#++++++++++++++++++++++++++++\n#partial loss\nfig, ax = plt.subplots()\nplt.tick_params(axis='y', which='both', labelleft='on', labelright='off')\nplt.xlabel('$iteration$', fontsize = font)\nplt.ylabel('$loss \\,\\, values$', fontsize = font)\nplt.yscale('log')\nplt.grid(True)\niteration = [total_record[i][0] for i in range(len(total_record))]\nloss_his  = [total_record[i][1] for i in range(len(total_record))]\nloss_b  = [total_record[i][2] for i in range(len(total_record))]\nloss_v  = [total_record[i][3] for i in range(len(total_record))]\nplt.plot(iteration, loss_his,'green',label=\"loss\")\nplt.plot(iteration, loss_b,'blue',label=\"boundary_loss\")\nplt.plot(iteration, loss_v, 'violet',label=\"variational_loss\")\nplt.legend(loc=\"upper right\")\nplt.tick_params( labelsize = 20)\nfig.set_size_inches(w=11,h=5.5)\nplt.show()\n\n\npnt_skip = 25\nfig, ax = plt.subplots()\nplt.locator_params(axis='x', nbins=6)\nplt.locator_params(axis='y', nbins=8)\nplt.xlabel('$x$', fontsize = font)\nplt.ylabel('$u$', fontsize = font)\nplt.axhline(0, linewidth=0.8, linestyle='-', color='gray')\nfor xc in grid:\n    plt.axvline(x=xc, linewidth=2, ls = '--')\nplt.plot(X_test, u_test, linewidth=1, color='r', label=''.join(['$exact$']))\nplt.plot(X_test[0::pnt_skip], u_pred[0::pnt_skip], 'k*', label='$VPINN$')\nplt.tick_params( labelsize = 20)\nlegend = plt.legend(shadow=True, loc='upper left', fontsize=18, ncol = 1)\nfig.set_size_inches(w=11,h=5.5)\nplt.show()\n# plt.savefig('prediction.pdf')\n#++++++++++++++++++++++++++++\n\nfig, ax = plt.subplots()\nplt.locator_params(axis='x', nbins=6)\nplt.locator_params(axis='y', nbins=8)\nplt.xlabel('$x$', fontsize = font)\nplt.ylabel('point-wise error', fontsize = font)\nplt.yscale('log')\nplt.axhline(0, linewidth=0.8, linestyle='-', color='gray')\nfor xc in grid:\n    plt.axvline(x=xc, linewidth=2, ls = '--')\nplt.plot(X_test, abs(u_test - u_pred), 'k')\nplt.tick_params( labelsize = 20)\nfig.set_size_inches(w=11,h=5.5)\nplt.show()\n\n#it may take a while to little bit to start","metadata":{"execution":{"iopub.execute_input":"2023-04-09T19:31:24.578837Z","iopub.status.busy":"2023-04-09T19:31:24.578100Z","iopub.status.idle":"2023-04-09T19:31:54.770554Z","shell.execute_reply":"2023-04-09T19:31:54.769613Z"},"papermill":{"duration":30.211007,"end_time":"2023-04-09T19:31:54.783811","exception":false,"start_time":"2023-04-09T19:31:24.572804","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}