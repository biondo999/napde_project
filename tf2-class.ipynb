{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyDOE","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:13:08.646185Z","iopub.execute_input":"2023-05-14T13:13:08.646613Z","iopub.status.idle":"2023-05-14T13:13:26.837892Z","shell.execute_reply.started":"2023-05-14T13:13:08.646582Z","shell.execute_reply":"2023-05-14T13:13:26.835887Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting pyDOE\n  Downloading pyDOE-0.3.8.zip (22 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pyDOE) (1.23.5)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pyDOE) (1.9.3)\nBuilding wheels for collected packages: pyDOE\n  Building wheel for pyDOE (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyDOE: filename=pyDOE-0.3.8-py3-none-any.whl size=18184 sha256=7cac4a5f07506d141f842e9cc73309854cbd7b1874ab9fc6b1099e9533b3eac7\n  Stored in directory: /root/.cache/pip/wheels/ce/b6/d7/c6b64746dba6433c593e471e0ac3acf4f36040456d1d160d17\nSuccessfully built pyDOE\nInstalling collected packages: pyDOE\nSuccessfully installed pyDOE-0.3.8\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"#you need to add a dataset with GaussJacobiQuadRule_V3 on the right \nimport sys\nsys.path.insert(1, '/kaggle/input/quadrule')\nfrom GaussJacobiQuadRule_V3 import Jacobi, DJacobi, GaussLobattoJacobiWeights, GaussJacobiWeights\n\n#import GaussJacobiQuadRule_V3\n\n\nimport tensorflow as tf\n#tf.disable_v2_behavior()\nimport pyDOE\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom pyDOE import lhs\n#from GaussJacobiQuadRule_V3.py import Jacobi, DJacobi, GaussLobattoJacobiWeights, GaussJacobiWeights\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-14T13:13:26.840247Z","iopub.execute_input":"2023-05-14T13:13:26.840631Z","iopub.status.idle":"2023-05-14T13:13:35.929242Z","shell.execute_reply.started":"2023-05-14T13:13:26.840595Z","shell.execute_reply":"2023-05-14T13:13:35.927956Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"class parameters:\n    def __init__(self,LR,Opt_Niter,N_Element,N_testfcn,N_Quad,lossb_weight,a,b):\n        self.LR = LR\n        self.Opt_Niter = Opt_Niter + 1\n        self.Opt_tresh = 2e-32\n        self.var_form=2\n        self.N_Element = N_Element\n        self.N_testfcn = N_testfcn #number of test function\n        self.N_Quad = N_Quad    #quadrature points \n        self.lossb_weight = lossb_weight #hyprparameter we may turn it up\n\n        #params for the exact sol and boundary term\n        self.omega = 8*np.pi\n        self.amp = 1\n        self.r1 = 80\n\n        #ingredients for loss\n        [self.x_quad, self.w_quad] = GaussLobattoJacobiWeights(N_Quad, 0, 0)\n        print(self.x_quad.dtype)\n\n        self.test_quad_element = self.Test_fcn(self.N_testfcn, self.x_quad)\n        self.d1test_quad_element, self.d2test_quad_element = self.dTest_fcn(self.N_testfcn, self.x_quad)\n\n\n        [self.x_l, self.x_r] = [a, b]     #modify in the future if you want general (a,b) interval\n        self.delta_x = (self.x_r - self.x_l)/N_Element\n        self.grid = np.asarray([ self.x_l + i*self.delta_x for i in range(self.N_Element+1)])\n\n        #build right side (it doesnt depend on the network)\n\n        self.F_ext_total = []\n        for e in range(self.N_Element):\n            x_quad_element = self.grid[e] + (self.grid[e+1]-self.grid[e])/2*(self.x_quad+1)  #traslation of the element \n\n            jacobian = (self.grid[e+1]-self.grid[e])/2\n            testfcn_element = np.asarray([ self.Test_f(n,self.x_quad)  for n in range(1, self.N_testfcn+1)])\n\n            #this is fh calculated on the real nodal values summed over each real nodal values(which comes form CGL nodes )\n\n            f_quad_element = self.f_ext(x_quad_element)\n            F_ext_element  = jacobian*np.asarray([sum(self.w_quad*f_quad_element*testfcn_element[i]) for i in range(self.N_testfcn)])    \n            F_ext_element = F_ext_element[:,None] \n            self.F_ext_total.append(F_ext_element)\n\n        self.F_ext_total = np.asarray(self.F_ext_total)\n\n\n        self.X_bound = np.asarray([a,b],dtype=np.float64)[:,None]\n        self.u_bound   = self.u_exact(self.X_bound)\n\n    def u_exact(self,x):\n        utemp = 0.1*np.sin(self.omega*x) + np.tanh(self.r1*x)\n        return self.amp*utemp\n\n    def f_ext(self,x):\n        gtemp =  -0.1*(self.omega**2)*np.sin(self.omega*x) - (2*self.r1**2)*(np.tanh(self.r1*x))/((np.cosh(self.r1*x))**2)\n        return -self.amp*gtemp\n\n    def Test_f(self,n,x):\n        test  = Jacobi(n+1,0,0,x) - Jacobi(n-1,0,0,x)\n        return test\n    \n    def Test_fcn(self, N_test,x):\n        test_total = []\n        for n in range(1,N_test+1):  \n            test  = Jacobi(n+1,0,0,x) - Jacobi(n-1,0,0,x)\n            test_total.append(test)\n        return np.asarray(test_total)\n\n    def dTest_fcn(self, N_test,x):  #valuete the first and second derivatives of test functions on a point x \n        d1test_total = []\n        d2test_total = []\n        for n in range(1,N_test+1):  \n            if n==1:\n                d1test = ((n+2)/2)*Jacobi(n,1,1,x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1,2,2,x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)\n            elif n==2:\n                d1test = ((n+2)/2)*Jacobi(n,1,1,x) - ((n)/2)*Jacobi(n-2,1,1,x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1,2,2,x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)    \n            else:\n                d1test = ((n+2)/2)*Jacobi(n,1,1,x) - ((n)/2)*Jacobi(n-2,1,1,x)\n                d2test = ((n+2)*(n+3)/(2*2))*Jacobi(n-1,2,2,x) - ((n)*(n+1)/(2*2))*Jacobi(n-3,2,2,x)\n                d1test_total.append(d1test)\n                d2test_total.append(d2test)    \n        return np.asarray(d1test_total), np.asarray(d2test_total)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:39:39.352557Z","iopub.execute_input":"2023-05-14T13:39:39.353323Z","iopub.status.idle":"2023-05-14T13:39:39.384618Z","shell.execute_reply.started":"2023-05-14T13:39:39.353275Z","shell.execute_reply":"2023-05-14T13:39:39.383338Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"params=parameters(0.01,1000,20,30,100,1,-1,1)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:13:52.622410Z","iopub.execute_input":"2023-05-14T13:13:52.623082Z","iopub.status.idle":"2023-05-14T13:13:53.257895Z","shell.execute_reply.started":"2023-05-14T13:13:52.623047Z","shell.execute_reply":"2023-05-14T13:13:53.256942Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/special/orthogonal.py:139: RuntimeWarning: invalid value encountered in multiply\n  np.poly1d.__init__(self, poly.coeffs * float(kn))\n","output_type":"stream"},{"name":"stdout","text":"float64\n","output_type":"stream"}]},{"cell_type":"code","source":"class Model(tf.keras.Model):\n  def __init__(self,*kwargs):\n    super().__init__()\n    #structure of the nework maybe implment a general way\n    self.dense1 = tf.keras.layers.Dense(units=16,\n                                        activation='relu',\n                                        kernel_initializer=tf.keras.initializers.HeNormal(),\n                                        bias_initializer=tf.keras.initializers.HeNormal())\n    self.dense2 = tf.keras.layers.Dense(units=16,\n                                    activation='relu',\n                                    kernel_initializer=tf.keras.initializers.HeNormal(),\n                                    bias_initializer=tf.keras.initializers.HeNormal())\n    self.y = tf.keras.layers.Dense(1)\n    self.history=[]\n    self.params=params\n    \n  def call(self, x, training=True):\n    x = x[:, tf.newaxis]\n    x = self.dense1(x)\n    x = self.dense2(x)\n    x = self.y(x)\n    return tf.squeeze(x, axis=1)\n\n  @tf.function\n  def net_du(self, x):\n    with tf.GradientTape() as g:\n      g.watch(x)\n      with tf.GradientTape() as gg:\n        gg.watch(x)\n        y = model(x)\n      dy_dx = gg.gradient(y, x)  # dy_dx = 2 * x\n    d2y_dx2 = g.gradient(dy_dx, x)  # d2y_dx2 = 2\n    return dy_dx,d2y_dx2\n\n  def train(self):\n    variables = self.variables\n    optimizer = tf.optimizers.Adam(learning_rate=self.params.LR)\n    print('-->training_starting')\n    for step in range(self.params.Opt_Niter):\n      with tf.GradientTape() as tape:\n        tape.watch(variables)\n        loss =self.calculate_loss(variables)\n        \n      gradient = tape.gradient(loss, variables)\n      optimizer.apply_gradients(zip(gradient, variables))\n\n      if step % 10 == 0:\n        print(f'Iter: {step}',f'loss: {loss.numpy():0.6f}')\n        self.history.append(loss)\n        \n  @tf.function    \n  def calculate_loss(self,variables):\n    varloss_total = 0\n    \n    for e in range(self.params.N_Element):\n        F_ext_element  = self.params.F_ext_total[e]\n        Ntest_element  = np.shape(F_ext_element)[0] #for each you have N_testfcn\n        x_quad_element = tf.constant(self.params.grid[e] + (self.params.grid[e+1]-self.params.grid[e])/2*(self.params.x_quad+1))\n        x_b_element    = tf.constant(np.array([[self.params.grid[e]], [self.params.grid[e+1]]]))\n        #to change change integral to the ref segment in (-1,1)\n        jacobian       = (self.params.grid[e+1]-self.params.grid[e])/2\n\n        u_NN_quad_element = model(x_quad_element)\n        \n        d1u_NN_quad_element, d2u_NN_quad_element = self.net_du(x_quad_element)\n\n\n    \n        \"\"\"\"\n        if self.params.var_form == 1:\n            U_NN_element = tf.reshape(tf.stack([-jacobian*tf.reduce_sum(self.wquad*d2u_NN_quad_element*test_quad_element[i]) \\\n                                               for i in range(Ntest_element)]),(-1,1))\n        \"\"\"\n        if self.params.var_form == 2:\n            U_NN_element = tf.reshape(tf.stack([ tf.reduce_sum(self.params.w_quad*d1u_NN_quad_element*self.params.d1test_quad_element[i]) \\\n                                          for i in range(Ntest_element)]),(-1,1)) \n            #i think we are going to use this most of the times \n        \"\"\"\n        if self.params.var_form == 3:\n            U_NN_element = tf.reshape(tf.stack([-1/jacobian*tf.reduce_sum(self.wquad*u_NN_quad_element*d2test_quad_element[i]) \\\n                                               +1/jacobian*tf.reduce_sum(u_NN_bound_element*np.array([-d1test_bound_element[i][0], d1test_bound_element[i][-1]]))  \\\n                                               for i in range(Ntest_element)]),(-1,1))\n        \"\"\"\n        \n        Res_NN_element = U_NN_element - F_ext_element\n        loss_element = tf.reduce_mean(tf.square(Res_NN_element))\n        varloss_total = varloss_total + loss_element\n    \n\n    lossb = tf.reduce_mean(tf.square(model(self.params.X_bound) - self.params.u_bound))  #u_NN_pred is what your network has calc,while u_tf is the real value at the boundary\n    #two losses \n    \n\n    \n    return self.params.lossb_weight *lossb + tf.cast(varloss_total, tf.float32) #casting to fix bugs","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:55:13.085598Z","iopub.execute_input":"2023-05-14T13:55:13.086868Z","iopub.status.idle":"2023-05-14T13:55:13.112887Z","shell.execute_reply.started":"2023-05-14T13:55:13.086821Z","shell.execute_reply":"2023-05-14T13:55:13.111551Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"model=Model(params)","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:55:16.492990Z","iopub.execute_input":"2023-05-14T13:55:16.493436Z","iopub.status.idle":"2023-05-14T13:55:16.507184Z","shell.execute_reply.started":"2023-05-14T13:55:16.493403Z","shell.execute_reply":"2023-05-14T13:55:16.506147Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model.train()","metadata":{"execution":{"iopub.status.busy":"2023-05-14T13:55:17.847025Z","iopub.execute_input":"2023-05-14T13:55:17.847502Z","iopub.status.idle":"2023-05-14T13:56:14.231079Z","shell.execute_reply.started":"2023-05-14T13:55:17.847466Z","shell.execute_reply":"2023-05-14T13:56:14.229130Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"-->training_starting\nIter: 0 loss: 570.843750\nIter: 10 loss: 570.843750\nIter: 20 loss: 570.843750\nIter: 30 loss: 570.843750\nIter: 40 loss: 570.843750\nIter: 50 loss: 570.843750\nIter: 60 loss: 570.843750\nIter: 70 loss: 570.843750\nIter: 80 loss: 570.843750\nIter: 90 loss: 570.843750\nIter: 100 loss: 570.843750\nIter: 110 loss: 570.843750\nIter: 120 loss: 570.843750\nIter: 130 loss: 570.843750\nIter: 140 loss: 570.843750\nIter: 150 loss: 570.843750\nIter: 160 loss: 570.843750\nIter: 170 loss: 570.843750\nIter: 180 loss: 570.843750\nIter: 190 loss: 570.843750\nIter: 200 loss: 570.843750\nIter: 210 loss: 570.843750\nIter: 220 loss: 570.843750\nIter: 230 loss: 570.843750\nIter: 240 loss: 570.843750\nIter: 250 loss: 570.843750\nIter: 260 loss: 570.843750\nIter: 270 loss: 570.843750\nIter: 280 loss: 570.843750\nIter: 290 loss: 570.843750\nIter: 300 loss: 570.843750\nIter: 310 loss: 570.843750\nIter: 320 loss: 570.843750\nIter: 330 loss: 570.843750\nIter: 340 loss: 570.843750\nIter: 350 loss: 570.843750\nIter: 360 loss: 570.843750\nIter: 370 loss: 570.843750\nIter: 380 loss: 570.843750\nIter: 390 loss: 570.843750\nIter: 400 loss: 570.843750\nIter: 410 loss: 570.843750\nIter: 420 loss: 570.843750\nIter: 430 loss: 570.843750\nIter: 440 loss: 570.843750\nIter: 450 loss: 570.843750\nIter: 460 loss: 570.843750\nIter: 470 loss: 570.843750\nIter: 480 loss: 570.843750\nIter: 490 loss: 570.843750\nIter: 500 loss: 570.843750\nIter: 510 loss: 570.843750\nIter: 520 loss: 570.843750\nIter: 530 loss: 570.843750\nIter: 540 loss: 570.843750\nIter: 550 loss: 570.843750\nIter: 560 loss: 570.843750\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[27], line 42\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m     41\u001b[0m   tape\u001b[38;5;241m.\u001b[39mwatch(variables)\n\u001b[0;32m---> 42\u001b[0m   loss \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m gradient \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, variables)\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(gradient, variables))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1760\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1756\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_override_gradient_function(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m       {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function(),\n\u001b[1;32m   1758\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatefulPartitionedCall\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gradient_function()}):\n\u001b[1;32m   1759\u001b[0m     flat_outputs \u001b[38;5;241m=\u001b[39m forward_function\u001b[38;5;241m.\u001b[39mcall(ctx, args_with_tangents)\n\u001b[0;32m-> 1760\u001b[0m \u001b[43mforward_backward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1289\u001b[0m, in \u001b[0;36m_ForwardBackwardCall.record\u001b[0;34m(self, flat_outputs)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Given outputs from the execution of `forward`, records the operation.\"\"\"\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape_watching\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(flat_outputs, ops\u001b[38;5;241m.\u001b[39mOperation)\n\u001b[1;32m   1286\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m flat_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1287\u001b[0m   \u001b[38;5;66;03m# We only record function calls which have outputs, and then only when a\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m   \u001b[38;5;66;03m# tape is watching.\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_functions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecord\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m      \u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_tangents\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1105\u001b[0m, in \u001b[0;36m_TapeGradientFunctions.record\u001b[0;34m(self, flat_outputs, inference_args, input_tangents)\u001b[0m\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecord\u001b[39m(\u001b[38;5;28mself\u001b[39m, flat_outputs, inference_args, input_tangents):\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Record the function call operation.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \n\u001b[1;32m   1094\u001b[0m \u001b[38;5;124;03m  For backprop, indicates the backward function to use and which new Tensors\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;124;03m      operation.\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m   backward_function, to_record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_backward_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forwardprop_output_indices:\n\u001b[1;32m   1108\u001b[0m     tape\u001b[38;5;241m.\u001b[39mrecord_operation_backprop_only(\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1110\u001b[0m         to_record, inference_args,\n\u001b[1;32m   1111\u001b[0m         backward_function)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1048\u001b[0m, in \u001b[0;36m_TapeGradientFunctions._wrap_backward_function\u001b[0;34m(self, forward_graph, backward, outputs)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainable_recorded_outputs \u001b[38;5;241m<\u001b[39m backward_function_inputs:\n\u001b[1;32m   1047\u001b[0m   recorded_outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m-> 1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mbackprop_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIsTrainable\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1049\u001b[0m   trainable_recorded_outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop_util.py:59\u001b[0m, in \u001b[0;36mIsTrainable\u001b[0;34m(tensor_or_dtype)\u001b[0m\n\u001b[1;32m     55\u001b[0m dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\n\u001b[1;32m     56\u001b[0m trainable_dtypes \u001b[38;5;241m=\u001b[39m [dtypes\u001b[38;5;241m.\u001b[39mfloat16, dtypes\u001b[38;5;241m.\u001b[39mfloat32, dtypes\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[1;32m     57\u001b[0m                     dtypes\u001b[38;5;241m.\u001b[39mcomplex64, dtypes\u001b[38;5;241m.\u001b[39mcomplex128, dtypes\u001b[38;5;241m.\u001b[39mresource,\n\u001b[1;32m     58\u001b[0m                     dtypes\u001b[38;5;241m.\u001b[39mvariant, dtypes\u001b[38;5;241m.\u001b[39mbfloat16]\n\u001b[0;32m---> 59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_quantized_dtypes_training\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     60\u001b[0m   trainable_dtypes\u001b[38;5;241m.\u001b[39mextend([dtypes\u001b[38;5;241m.\u001b[39mqint8, dtypes\u001b[38;5;241m.\u001b[39mqint16, dtypes\u001b[38;5;241m.\u001b[39mqint32,\n\u001b[1;32m     61\u001b[0m                            dtypes\u001b[38;5;241m.\u001b[39mquint8, dtypes\u001b[38;5;241m.\u001b[39mquint16])\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mbase_dtype \u001b[38;5;129;01min\u001b[39;00m trainable_dtypes\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"t=np.array([1,2,3])\nmodel(t)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T12:48:22.151816Z","iopub.execute_input":"2023-05-13T12:48:22.152201Z","iopub.status.idle":"2023-05-13T12:48:22.166753Z","shell.execute_reply.started":"2023-05-13T12:48:22.152172Z","shell.execute_reply":"2023-05-13T12:48:22.165760Z"},"trusted":true},"execution_count":null,"outputs":[]}]}